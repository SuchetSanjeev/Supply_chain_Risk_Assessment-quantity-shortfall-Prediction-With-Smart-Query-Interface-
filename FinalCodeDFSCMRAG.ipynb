{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e94139-79fb-4ea2-a7a3-c409b58ac307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import statsmodels.api as sm                                                                                    # p value check\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor                                      # feature importance and VIF\n",
    "import joblib                                                                                                   # pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190afecc-0fc6-4c54-b91c-a15a1d6b5c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Dataset ===\n",
    "def load_dataset(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd6681-71a0-4682-9e1f-cee43cd5c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Feature Engineering ===\n",
    "def feature_engineering(df):\n",
    "    df['Order_Pressure'] = df['Ordered_Qty'] / df['Committed_Lead_Days']\n",
    "    df['Vendor_Risk'] = (1 - df['Reliability_score'] / 100) * df['Quality_Rejection_Rate (%)']\n",
    "    df['Transit_Risk'] = df['Avg_Transit_Days'] * df['Weather_Disruption_Index'] * df['Route Risk Score']\n",
    "    df['Demand_vs_Reliability'] = df['Ordered_Qty'] / (df['Reliability_score'] + 1)\n",
    "    df['Stress_Score'] = df['Transit_Risk'] * df['Order_Pressure']\n",
    "\n",
    "    df['High_Risk_Vendor'] = (df['Vendor_Risk'] > 1.5).astype(int)\n",
    "    df['High_Order_Pressure'] = (df['Order_Pressure'] > 20).astype(int)\n",
    "    df['High_Transit_Risk'] = (df['Transit_Risk'] > 25).astype(int)\n",
    "    df['High_Demand_vs_Reliability'] = (df['Demand_vs_Reliability'] > 8).astype(int)\n",
    "    df['High_Stress_Score'] = (df['Stress_Score'] > 400).astype(int)\n",
    "\n",
    "    df['Low_Order_Pressure'] = (df['Order_Pressure'] < 6.2).astype(int)\n",
    "    df['Low_Stress_Score'] = (df['Stress_Score'] < 40).astype(int)\n",
    "    df['Low_Demand_vs_Reliability'] = (df['Demand_vs_Reliability'] < 3).astype(int)\n",
    "    df['Low_Vendor_Risk'] = (df['Vendor_Risk'] < 0.5).astype(int)\n",
    "\n",
    "    df['Price_per_Unit_vs_Order'] = df['Price_per_Unit'] / (df['Ordered_Qty'] + 1)\n",
    "    df['Low_Price_per_Unit_vs_Order'] = (df['Price_per_Unit_vs_Order'] < 0.05).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c57b98-fdec-4ab2-b68b-061e76ea7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Drop Columns ===\n",
    "def drop_unnecessary_columns(df):\n",
    "    drop_cols = [\n",
    "            'Order_ID',                  #not required\n",
    "            'Actual_Delivery_Date',      #not using\n",
    "            'Delivered_Qty',             #cannot be given\n",
    "            'Actual_Lead_Days',          #cannot be give\n",
    "            'Shortfall',                 #data leakage\n",
    "            'Shortfall_flag',            # Target â€” should be saved separately if needed\n",
    "            'Delay_Indicator',           #not required for shortfall\n",
    "            'vendor_name',               #not required\n",
    "            'Destination',               #not required\n",
    "            'Order_Date',                #not required\n",
    "            'Contractual_Delivery_Date', #not required\n",
    "            'vendor_id',                 # as two vendor ids columns are present\n",
    "            'Transit_Risk',              # High VIF, derived\n",
    "            'Vendor_Risk',               # High VIF, derived\n",
    "            'Weather_Disruption_Index',  # Already in use inside feature\n",
    "            'Route Risk Score',          # Already in use inside feature\n",
    "            'Ordered_Qty',               # Already in use inside feature\n",
    "            'Avg_Transit_Days',          # Already in use inside feature\n",
    "            'Demand_vs_Reliability',     # Already in use inside feature (High VIF)\n",
    "            'Reliability_score',         # Already in use inside feature (High VIF)\n",
    "            'Quality_Rejection_Rate (%)',# Already in use inside feature (High VIF)\n",
    "            'Committed_Lead_Days',       # Already in use inside feature (High VIF)\n",
    "            'avg_lead_days',             # dont need\n",
    "            'Supplier_Dependency_Index'  # didnt use yet\n",
    "    ]\n",
    "    return df.drop(columns=drop_cols, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db96b2-5594-4a0e-8332-464bf45ddda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(df):\n",
    "    # Convert high-cardinality categorical variables to numeric codes\n",
    "    id_cols = ['Component_ID', 'Vendor_ID', 'Route_ID', 'Source']\n",
    "    for col in id_cols:\n",
    "        df[col] = df[col].astype('category').cat.codes\n",
    "\n",
    "    # Ordinal Encoding for Peak_Congestion_Indicator\n",
    "    ordinal_map = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "    df['Peak_Congestion_Indicator'] = df['Peak_Congestion_Indicator'].map(ordinal_map)\n",
    "\n",
    "    # One-Hot Encoding for low-cardinality categorical variables\n",
    "    cat_cols = ['Mode', 'Backup Route Availability']\n",
    "    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78beac88-03a4-461b-97e1-d537f5451da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Split and Scale Data ===\n",
    "def split_and_scale_data(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da689e-85d5-472a-b4ac-d6273d582dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Apply SMOTE ===\n",
    "def apply_smote(X_train_scaled, y_train):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    return sm.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966c715-e262-4d68-9e2a-21a82ef192e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dimensionality Reduction ===\n",
    "def apply_pca(X_train, X_test, n_components=10):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    return X_train_pca, X_test_pca\n",
    "\n",
    "def apply_fa(X_train, X_test, n_components=10):\n",
    "    fa = FactorAnalysis(n_components=n_components, random_state=42)\n",
    "    X_train_fa = fa.fit_transform(X_train)\n",
    "    X_test_fa = fa.transform(X_test)\n",
    "    return X_train_fa, X_test_fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99363493-c5da-4610-a3a0-3d97c5539d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot ROC Curve ===\n",
    "def plot_roc_curve(y_test, y_probs, model_name):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "    auc = roc_auc_score(y_test, y_probs)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b0726b-1775-4e5f-939d-66c98a23c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def train_and_evaluate_model(model, model_name, X_train_res, y_train_res, X_test_scaled, y_test):\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "\n",
    "    # Predict directly (no threshold tuning)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Probabilities for ROC\n",
    "    y_probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # Evaluation\n",
    "    print(f\"\\n===== {model_name} =====\")\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    plot_roc_curve(y_test, y_probs, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962c5d0-8e31-4cb5-8137-c59bdb1dc70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# === P-Value Checker ===\n",
    "def run_p_value_check(X, y):\n",
    "    X = X.astype(float)\n",
    "    X_const = sm.add_constant(X)\n",
    "    logit_model = sm.Logit(y, X_const)\n",
    "    result = logit_model.fit()\n",
    "    print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a625ad2-f334-46f8-a04b-3cfd4f950f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model, X, top_n=20):\n",
    "    \"\"\"\n",
    "    Plots and prints top N feature importances for tree-based models.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Print top N as table\n",
    "    print(f\"\\nðŸ“Š Top {top_n} Feature Importances:\")\n",
    "    print(importance_df.head(top_n).to_string(index=False))\n",
    "\n",
    "    # Plot top N\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(top_n))\n",
    "    plt.title(\"Top Feature Importances\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a165379c-c0ca-4be9-9d2b-e0e6926ae27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(X):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with Variance Inflation Factors for all features.\n",
    "    \"\"\"\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    import pandas as pd\n",
    "\n",
    "    X = X.copy()\n",
    "    X = X.astype(float)  # ensure numerical\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return vif_data.sort_values(by=\"VIF\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff244b-d7aa-4a9b-8316-332854c08473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Main Pipeline ===\n",
    "def main():\n",
    "    df = load_dataset(\"full_merged_data_v1.csv\")\n",
    "    df = feature_engineering(df)\n",
    "    target = df['Shortfall_flag'].copy()\n",
    "    df = drop_unnecessary_columns(df)\n",
    "    df_encoded = encode_features(df)\n",
    "    # df_encoded = encode_features(df, target) # for target encoding\n",
    "    \n",
    "    X = df_encoded.copy()\n",
    "    y = target\n",
    "    # run_p_value_check(X, y)\n",
    "\n",
    "    # print(\"\\n--- VIF Scores ---\")\n",
    "    # print(calculate_vif(X))\n",
    "    \n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, scaler = split_and_scale_data(X, y)\n",
    "    X_train_res, y_train_res = apply_smote(X_train_scaled, y_train)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#     model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "#     model.fit(X_train_res,y_train_res)\n",
    "#     joblib.dump(model, \"model2.pkl\")\n",
    "#     joblib.dump(scaler, \"scaler2.pkl\")\n",
    "#     joblib.dump(X.columns.tolist(), \"columns2.pkl\")\n",
    "#     print(\"âœ… Model2, scaler2, and training columns2 saved using joblib.\")\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "    # train_and_evaluate_model(rf_model, \"Random Forest\", X_train_res, y_train_res, X_test_scaled, y_test)\n",
    "    # get_feature_importance(rf_model, pd.DataFrame(X_train_res, columns=X.columns))\n",
    "\n",
    "    # # === Apply PCA or FA ===\n",
    "    # X_train_reduced, X_test_reduced = apply_pca(X_train_res, X_test_scaled, n_components=15)\n",
    "    # X_train_reduced, X_test_reduced = apply_fa(X_train_res, X_test_scaled, n_components=15)\n",
    "\n",
    "    # Compare baseline model vs PCA model\n",
    "    #Without PCA/FA\n",
    "    train_and_evaluate_model(RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42), \"Without PCA/FA Random Forest\", X_train_res, y_train_res, X_test_scaled, y_test)\n",
    "    # train_and_evaluate_model(LogisticRegression(penalty='l1',solver='liblinear',C=1.0,random_state=42), \"Without PCA/FA Logistic Regression\", X_train_res, y_train_res, X_test_scaled, y_test)\n",
    "    # train_and_evaluate_model(XGBClassifier(n_estimators=100,learning_rate=0.1,max_depth=6,reg_alpha=0.5), \"Without PCA/FA XGBoost\", X_train_res, y_train_res, X_test_scaled, y_test)\n",
    "    # train_and_evaluate_model(SVC(probability=True), \"Without PCA/FA SVM\", X_train_res, y_train_res, X_test_scaled, y_test)\n",
    "\n",
    "    #With PCA/FA\n",
    "    # train_and_evaluate_model(RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42), \"With PCA/FA Random Forest\", X_train_reduced, y_train_res, X_test_reduced, y_test)\n",
    "    # train_and_evaluate_model(LogisticRegression(penalty='l1',solver='liblinear',C=1.0,random_state=42), \"With PCA/FA Logistic Regression\", X_train_reduced, y_train_res, X_test_reduced, y_test)\n",
    "    # train_and_evaluate_model(XGBClassifier(n_estimators=100,learning_rate=0.1,max_depth=6,reg_alpha=0.5), \"With PCA/FA XGBoost\", X_train_reduced, y_train_res, X_test_reduced, y_test)\n",
    "    # train_and_evaluate_model(SVC(probability=True), \"With PCA/FA SVM\", X_train_reduced, y_train_res, X_test_reduced, y_test)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e673f-ec81-4430-88c9-acbf591c9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------THE END---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52140082-e4c6-471e-a926-ccf7ad363ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling\n",
    "# === To Save Model, Scaler, and Columns using joblib ===\n",
    "# import joblib\n",
    "# model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "# model.fit(X_train_res,y_train_res)\n",
    "# joblib.dump(model, \"model.pkl\")\n",
    "# joblib.dump(scaler, \"scaler.pkl\")\n",
    "# joblib.dump(X.columns.tolist(), \"columns.pkl\")\n",
    "# print(\"âœ… Model, scaler, and training columns saved using joblib.\")\n",
    "\n",
    "# #for loading model later\n",
    "\n",
    "# âœ… Save final dataset before scaling\n",
    "# df_encoded['Shortfall_flag'] = target\n",
    "# df_encoded.to_csv(\"finalDataset.csv\", index=False)\n",
    "# print(\"âœ… Final dataset saved as 'finalDataset.csv'\")\n",
    "\n",
    "# Save balanced data with predictions\n",
    "# save_final_balanced_dataset(X_train_res, y_train_res, X.columns.tolist(), scaler, model)\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "# model.fit(X_train_res,y_train_res)\n",
    "# joblib.dump(model, \"model.pkl\")\n",
    "# joblib.dump(scaler, \"scaler.pkl\")\n",
    "# joblib.dump(X.columns.tolist(), \"columns.pkl\")\n",
    "# print(\"âœ… Model, scaler, and training columns saved using joblib.\")\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "# model.fit(X_train_res,y_train_res)\n",
    "# save_test_predictions(X_test, y_test, X.columns.tolist(), model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742ae700-9acf-43b5-bd43-a66a3e5c8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #FOR CHANDAN \n",
    "# def save_final_balanced_dataset(X_train_res, y_train_res, reference_columns, scaler, model, filename='final_balanced_with_predictions.csv'):\n",
    "#     # Convert resampled X to DataFrame\n",
    "#     X_res_df = pd.DataFrame(X_train_res, columns=reference_columns)\n",
    "#     # Predict on balanced data\n",
    "#     probs = model.predict_proba(X_train_res)[:, 1]\n",
    "#     preds = (probs >= 0.5).astype(int)\n",
    "#     # Add predictions and target\n",
    "#     X_res_df['Predicted_Shortfall_Flag'] = preds\n",
    "#     X_res_df['True_Shortfall_Flag'] = y_train_res.reset_index(drop=True)\n",
    "#     # Save to CSV\n",
    "#     X_res_df.to_csv(filename, index=False)\n",
    "#     print(f\"âœ… Final balanced dataset with predictions saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1e786-af18-4587-8907-7cbdc6bbe06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #FOR CHANDAN\n",
    "# def save_test_predictions(X_test, y_test, reference_columns, model, filename='test_predictions.csv'):\n",
    "#     import pandas as pd\n",
    "\n",
    "#     # Predict probabilities and labels\n",
    "#     probs = model.predict_proba(X_test)[:, 1]\n",
    "#     preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "#     # Convert X_test_scaled to DataFrame\n",
    "#     X_test_df = pd.DataFrame(X_test, columns=reference_columns)\n",
    "\n",
    "#     # Add prediction columns\n",
    "#     X_test_df['Predicted_Shortfall_Flag'] = preds\n",
    "#     X_test_df['Probability_Shortfall'] = probs\n",
    "\n",
    "#     # Save to CSV\n",
    "#     X_test_df.to_csv(filename, index=False)\n",
    "#     print(f\"âœ… Test predictions saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679eacc5-28b8-4bb8-9a5b-f5593c9f793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # === Encode Categorical Columns ===\n",
    "# def encode_features(df):\n",
    "#     # Frequency Encoding for high-cardinality categorical variables\n",
    "#     freq_encode_cols = ['Component_ID', 'Vendor_ID', 'Route_ID', 'Source']\n",
    "#     for col in freq_encode_cols:\n",
    "#         freq_map = df[col].value_counts().to_dict()\n",
    "#         df[col] = df[col].map(freq_map)\n",
    "\n",
    "#     # Ordinal Encoding\n",
    "#     ordinal_map = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "#     df['Peak_Congestion_Indicator'] = df['Peak_Congestion_Indicator'].map(ordinal_map)\n",
    "\n",
    "#     # One-Hot Encoding for low-cardinality categorical variables\n",
    "#     cat_cols = ['Mode', 'Backup Route Availability']\n",
    "#     return pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# #Target encoding\n",
    "# # === Encode Categorical Columns ===\n",
    "# def encode_features(df, target):\n",
    "#     # Ensure target column exists in df\n",
    "#     df['_target_temp_'] = target.values\n",
    "\n",
    "#     # Target Encoding for high-cardinality categorical variables\n",
    "#     target_encode_cols = ['Component_ID', 'Vendor_ID', 'Route_ID', 'Source']\n",
    "#     for col in target_encode_cols:\n",
    "#         target_map = df.groupby(col)['_target_temp_'].mean().to_dict()\n",
    "#         df[col] = df[col].map(target_map)\n",
    "\n",
    "#     df.drop(columns=['_target_temp_'], inplace=True)\n",
    "\n",
    "#     # Ordinal Encoding\n",
    "#     ordinal_map = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "#     df['Peak_Congestion_Indicator'] = df['Peak_Congestion_Indicator'].map(ordinal_map)\n",
    "\n",
    "#     # One-Hot Encoding\n",
    "#     cat_cols = ['Mode', 'Backup Route Availability']\n",
    "#     return pd.get_dummies(df, columns=cat_cols, drop_first=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
